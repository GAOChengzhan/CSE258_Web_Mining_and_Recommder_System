{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13362e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import statistics\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66fe8aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assertFloat(x):\n",
    "    assert type(float(x)) == float\n",
    "\n",
    "def assertFloatList(items, N):\n",
    "    assert len(items) == N\n",
    "    assert [type(float(x)) for x in items] == [float]*N\n",
    "#store the answer\n",
    "answers = {}\n",
    "#read the data from file'5year.arff'\n",
    "f = open(\"data/5year.arff\", 'r')\n",
    "# Read and parse the data\n",
    "while not '@data' in f.readline():\n",
    "    pass\n",
    "\n",
    "dataset = []\n",
    "for l in f:\n",
    "    if '?' in l: # Missing entry\n",
    "        continue\n",
    "    l = l.split(',')\n",
    "    values = [1] + [float(x) for x in l]\n",
    "    values[-1] = values[-1] > 0 # Convert to bool\n",
    "    dataset.append(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63cbef40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9656878917848895, 0.47668514315934635]\n"
     ]
    }
   ],
   "source": [
    "#Task1----------------------------------------------------------------------------\n",
    "#build logistic model\n",
    "datasetNP=np.array(dataset)\n",
    "dataX=datasetNP[:,:-1]\n",
    "dataY=datasetNP[:,-1]\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model_Task1=LogisticRegression(C=1.0)\n",
    "model_Task1.fit(dataX,dataY)\n",
    "#calculate the accuracy and Balanced Error Rate (BER)\n",
    "def accuracy_BER_calculation(y,y_predict):\n",
    "    TruePt,TrueNg,FalsePt,FalseNg=0,0,0,0\n",
    "    for i in range(len(y_predict)):\n",
    "        if y_predict[i]==0:\n",
    "            if y[i]==0:\n",
    "                TrueNg+=1\n",
    "            else:\n",
    "                FalseNg+=1\n",
    "        else:\n",
    "            if y[i]==1:\n",
    "                TruePt+=1\n",
    "            else:\n",
    "                FalsePt+=1\n",
    "    BER=0.5*((FalsePt/(FalsePt+TrueNg))+(FalseNg/(FalseNg+TruePt)))\n",
    "    accuracy=(TrueNg+TruePt)/len(y)\n",
    "    return [accuracy,BER]\n",
    "dataY_pre1=model_Task1.predict(dataX)\n",
    "answers['Q1']= accuracy_BER_calculation(dataY,dataY_pre1)\n",
    "assertFloatList(answers['Q1'], 2)\n",
    "print(answers['Q1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49e64383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6961398878258, 0.30388977031577397]\n"
     ]
    }
   ],
   "source": [
    "#Task2----------------------------------------------------------------------------\n",
    "model_Task2=LogisticRegression(C=1.0,class_weight='balanced')\n",
    "model_Task2.fit(dataX,dataY)\n",
    "dataY_pre2=model_Task2.predict(dataX)\n",
    "answers['Q2']= accuracy_BER_calculation(dataY,dataY_pre2)\n",
    "assertFloatList(answers['Q2'], 2)\n",
    "print(answers['Q2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ada3736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29287226079549855, 0.3159203980099502, 0.2585616438356164]\n"
     ]
    }
   ],
   "source": [
    "#Task3----------------------------------------------------------------------------\n",
    "random.seed(3)\n",
    "random.shuffle(dataset)\n",
    "datasetNP3=np.array(dataset)\n",
    "dataX3=datasetNP3[:,:-1]\n",
    "dataY3=datasetNP3[:,-1]\n",
    "length=len(dataX3)\n",
    "Xtrain, Xvalid, Xtest = dataX3[:length//2], dataX3[length//2:(3*length)//4], dataX3[(3*length)//4:]\n",
    "ytrain, yvalid, ytest = dataY3[:length//2], dataY3[length//2:(3*length)//4], dataY3[(3*length)//4:]\n",
    "model_Task3=LogisticRegression(C=1.0,class_weight='balanced')\n",
    "model_Task3.fit(Xtrain,ytrain)\n",
    "ytrain_pre=model_Task3.predict(Xtrain)\n",
    "yvalid_pre=model_Task3.predict(Xvalid)\n",
    "ytest_pre=model_Task3.predict(Xtest)\n",
    "[berTrain, berValid, berTest]=[accuracy_BER_calculation(ytrain,ytrain_pre)[1],\\\n",
    "                               accuracy_BER_calculation(yvalid,yvalid_pre)[1],\\\n",
    "                               accuracy_BER_calculation(ytest,ytest_pre)[1]]\n",
    "answers['Q3'] = [berTrain, berValid, berTest]\n",
    "assertFloatList(answers['Q3'], 3)\n",
    "print(answers['Q3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6d832da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32406151062867483, 0.31931252826775214, 0.3301673450927182, 0.3227046585255541, 0.3159203980099502, 0.3111714156490276, 0.2955030044582283, 0.29618143050978873, 0.29618143050978873]\n"
     ]
    }
   ],
   "source": [
    "#Task4----------------------------------------------------------------------------\n",
    "C_val=[10**(-4),10**(-3),10**(-2),10**(-1),1,10,10**2,10**3,10**4]\n",
    "#Report the validation BER\n",
    "berList=[]\n",
    "for c in C_val:\n",
    "    model_Task4=LogisticRegression(C=c,class_weight='balanced')\n",
    "    model_Task4.fit(Xtrain,ytrain)\n",
    "    yvalid_pre=model_Task4.predict(Xvalid)\n",
    "    berList.append(accuracy_BER_calculation(yvalid,yvalid_pre)[1])\n",
    "answers['Q4'] = berList\n",
    "assertFloatList(answers['Q4'], 9)\n",
    "print(berList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42bf1cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26335616438356163, 0.25582191780821917, 0.26472602739726026, 0.26404109589041097, 0.2585616438356164, 0.2723091976516634, 0.26267123287671235, 0.26267123287671235, 0.26267123287671235]\n"
     ]
    }
   ],
   "source": [
    "#Task5----------------------------------------------------------------------------\n",
    "berList5=[]\n",
    "bestC=1.0\n",
    "for c in C_val:\n",
    "    model_Task5=LogisticRegression(C=c,class_weight='balanced')\n",
    "    model_Task5.fit(Xtrain,ytrain)\n",
    "    ytest_pre=model_Task5.predict(Xtest)\n",
    "    berList5.append(accuracy_BER_calculation(ytest,ytest_pre)[1])\n",
    "print(berList5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3845b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 0.26267123287671235]\n"
     ]
    }
   ],
   "source": [
    "bestC=100\n",
    "ber5=berList5[6]\n",
    "answers['Q5'] = [bestC, ber5]\n",
    "assertFloatList(answers['Q5'], 2)\n",
    "print(answers['Q5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc0c6e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task6----------------------------------------------------------------------------\n",
    "data_taskRecommend=pd.read_json('data/young_adult_10000.json', lines=True)\n",
    "dataTrain = data_taskRecommend[:9000]\n",
    "dataTest = data_taskRecommend[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a1d195a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id                          8842281e1d1347389f2ab93d60773d4d\n",
       "book_id                                                   2767052\n",
       "review_id                        248c011811e945eca861b5c31a549291\n",
       "rating                                                          5\n",
       "review_text     I cracked and finally picked this up. Very enj...\n",
       "date_added                         Wed Jan 13 13:38:25 -0800 2010\n",
       "date_updated                       Wed Mar 22 11:46:36 -0700 2017\n",
       "read_at                            Sun Mar 25 00:00:00 -0700 2012\n",
       "started_at                         Fri Mar 23 00:00:00 -0700 2012\n",
       "n_votes                                                        24\n",
       "n_comments                                                     25\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_taskRecommend.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d30c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerBook = defaultdict(set)\n",
    "booksPerUser = defaultdict(set)\n",
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerBook = defaultdict(list)\n",
    "globalAvg=0\n",
    "trainRatingDict = {} # To retrieve a rating for a specific user/item pair\n",
    "for i in range(len(dataTrain)):\n",
    "    d=dataTrain.iloc[i,:]\n",
    "    user,book = d['user_id'], d['book_id']\n",
    "    rating=d['rating']\n",
    "    globalAvg+=rating\n",
    "    usersPerBook[book].add(user)\n",
    "    booksPerUser[user].add(book)\n",
    "    reviewsPerUser[user].append(rating)\n",
    "    reviewsPerBook[book].append(rating)\n",
    "    trainRatingDict[(user,book)]=rating\n",
    "globalAvg=globalAvg/len(dataTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c549d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def JaccardSimilarity(set1,set2):\n",
    "    inter = len(set1.intersection(set2))\n",
    "    uni = len(set1.union(set2))\n",
    "    return inter/uni\n",
    "def mostKSimilar (targetedBook, K): \n",
    "    JSimList = []\n",
    "    users = usersPerBook[targetedBook] \n",
    "    for comparedBook in usersPerBook : \n",
    "        if targetedBook == comparedBook: \n",
    "            continue\n",
    "        similarity = JaccardSimilarity(users,usersPerBook[comparedBook])\n",
    "        JSimList.append((similarity ,comparedBook))\n",
    "    JSimList.sort(key=lambda x:x[0],reverse=True) # Sort to find the most similar\n",
    "    return JSimList[:K]\n",
    "answers['Q6'] = mostKSimilar(2767052, 10)\n",
    "assert len(answers['Q6']) == 10\n",
    "assertFloatList([x[0] for x in answers['Q6']], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ffae009",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.4125, 6148028),\n",
       " (0.3411764705882353, 7260188),\n",
       " (0.1590909090909091, 256683),\n",
       " (0.1375, 1162543),\n",
       " (0.11494252873563218, 11735983),\n",
       " (0.10989010989010989, 13335037),\n",
       " (0.10810810810810811, 28187),\n",
       " (0.10666666666666667, 428263),\n",
       " (0.09876543209876543, 49041),\n",
       " (0.09782608695652174, 41865)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers['Q6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7d9f395",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2514617362474678\n"
     ]
    }
   ],
   "source": [
    "#Task7----------------------------------------------------------------------------\n",
    "avgRatingPerBook=defaultdict(float)\n",
    "for book in reviewsPerBook:\n",
    "    ReviewList=reviewsPerBook[book]\n",
    "    avgRatingPerBook[book]=statistics.mean(ReviewList)\n",
    "#testset\n",
    "testRatingDict = {} \n",
    "for i in range(len(dataTest)):\n",
    "    d=dataTest.iloc[i,:]\n",
    "    user,book = d['user_id'], d['book_id']\n",
    "    rating=d['rating']\n",
    "    testRatingDict[(user,book)]=rating\n",
    "\n",
    "#calculate global average ratings of all books\n",
    "predictUserBook=[]\n",
    "sumSim=[]\n",
    "for (user,book) in testRatingDict:\n",
    "    numerator=0\n",
    "    denominator=0\n",
    "    users= usersPerBook[book] \n",
    "    for comparedBook in booksPerUser[user]:\n",
    "        sim_ij=0\n",
    "        if book == comparedBook: \n",
    "            continue\n",
    "        sim_ij = JaccardSimilarity(users,usersPerBook[comparedBook])\n",
    "        denominator+=(sim_ij)\n",
    "        numerator+=((trainRatingDict[(user,comparedBook)]-avgRatingPerBook[comparedBook])*sim_ij)\n",
    "    sumSim.append(denominator)\n",
    "    if  denominator!=0:\n",
    "        ans=(avgRatingPerBook[book]+(numerator/denominator))  \n",
    "    else:\n",
    "        ans= globalAvg\n",
    "    predictUserBook.append(ans)\n",
    "#calculate MSE\n",
    "SquaredError=0\n",
    "for i,(user,book) in enumerate(testRatingDict):\n",
    "    Error=testRatingDict[(user,book)]-predictUserBook[i]\n",
    "    SquaredError+=(Error**2)\n",
    "mse7=SquaredError/(len(testRatingDict))\n",
    "answers['Q7'] = mse7\n",
    "assertFloat(answers['Q7'])\n",
    "print(mse7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "865cdcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2461869156447403\n"
     ]
    }
   ],
   "source": [
    "#Task8----------------------------------------------------------------------------\n",
    "avgRatingPerUser=defaultdict(float)\n",
    "for user in reviewsPerUser:\n",
    "    ReviewList=reviewsPerUser[user]\n",
    "    avgRatingPerUser[user]=statistics.mean(ReviewList)\n",
    "predictUserBook=[]\n",
    "for (user,book) in testRatingDict:\n",
    "    numerator=0\n",
    "    denominator=0\n",
    "    books = booksPerUser[user] \n",
    "    for comparedUser in usersPerBook[book]:\n",
    "        if user == comparedUser: \n",
    "            continue\n",
    "        sim_uv = JaccardSimilarity(books,booksPerUser[comparedUser])\n",
    "        denominator+=(sim_uv)\n",
    "        numerator+=((trainRatingDict[(comparedUser,book)]-avgRatingPerUser[comparedUser])*sim_uv)\n",
    "    if  denominator!=0:\n",
    "        ans=(avgRatingPerUser[user]+(numerator/denominator))  \n",
    "    else:\n",
    "        ans= globalAvg\n",
    "    predictUserBook.append(ans)\n",
    "#calculate MSE\n",
    "SquaredError=0\n",
    "for i,(user,book) in enumerate(testRatingDict):\n",
    "    Error=testRatingDict[(user,book)]-predictUserBook[i]\n",
    "    SquaredError+=Error**2\n",
    "mse8=SquaredError/(len(testRatingDict))\n",
    "answers['Q8'] = mse8\n",
    "assertFloat(answers['Q8'])\n",
    "print(mse8)\n",
    "f = open(\"answers_hw2.txt\", 'w')\n",
    "f.write(str(answers) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cc8257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
