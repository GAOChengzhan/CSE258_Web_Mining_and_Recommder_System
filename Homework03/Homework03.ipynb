{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be1a566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import scipy.optimize\n",
    "from sklearn import svm\n",
    "import numpy\n",
    "import string\n",
    "import random\n",
    "import string\n",
    "from sklearn import linear_model\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def assertFloat(x):\n",
    "    assert type(float(x)) == float\n",
    "\n",
    "def assertFloatList(items, N):\n",
    "    assert len(items) == N\n",
    "    assert [type(float(x)) for x in items] == [float]*N\n",
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)\n",
    "def readCSV(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        u,b,r = l.strip().split(',')\n",
    "        r = int(r)\n",
    "        yield u,b,r\n",
    "answers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9080fac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rating baseline: compute averages for each user, or return the global average if we've never seen the user before\n",
    "\n",
    "allRatings = []\n",
    "userRatings = defaultdict(list)\n",
    "\n",
    "for user,book,r in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    r = int(r)\n",
    "    allRatings.append(r)\n",
    "    userRatings[user].append(r)\n",
    "\n",
    "globalAverage = sum(allRatings) / len(allRatings)\n",
    "userAverage = {}\n",
    "for u in userRatings:\n",
    "    userAverage[u] = sum(userRatings[u]) / len(userRatings[u])\n",
    "\n",
    "predictions = open(\"predictions_Rating.csv\", 'w')\n",
    "for l in open(\"pairs_Rating.csv\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "    #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split(',')\n",
    "    if u in userAverage:\n",
    "        predictions.write(u + ',' + b + ',' + str(userAverage[u]) + '\\n')\n",
    "    else:\n",
    "        predictions.write(u + ',' + b + ',' + str(globalAverage) + '\\n')\n",
    "\n",
    "predictions.close()\n",
    "\n",
    "### Would-read baseline: just rank which books are popular and which are not, and return '1' if a book is among the top-ranked\n",
    "\n",
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for user,book,_ in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    bookCount[book] += 1\n",
    "    totalRead += 1\n",
    "\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > totalRead/2: break\n",
    "\n",
    "predictions = open(\"predictions_Read.csv\", 'w')\n",
    "for l in open(\"pairs_Read.csv\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "    #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split(',')\n",
    "    if b in return1:\n",
    "        predictions.write(u + ',' + b + \",1\\n\")\n",
    "    else:\n",
    "        predictions.write(u + ',' + b + \",0\\n\")\n",
    "\n",
    "predictions.close()\n",
    "\n",
    "### Category prediction baseline: Just consider some of the most common words from each category\n",
    "\n",
    "catDict = {\n",
    "  \"children\": 0,\n",
    "  \"comics_graphic\": 1,\n",
    "  \"fantasy_paranormal\": 2,\n",
    "  \"mystery_thriller_crime\": 3,\n",
    "  \"young_adult\": 4\n",
    "}\n",
    "\n",
    "predictions = open(\"predictions_Category.csv\", 'w')\n",
    "predictions.write(\"userID,reviewID,prediction\\n\")\n",
    "for l in readGz(\"test_Category.json.gz\"):\n",
    "    cat = catDict['fantasy_paranormal'] # If there's no evidence, just choose the most common category in the dataset\n",
    "    words = l['review_text'].lower()\n",
    "    if 'children' in words:\n",
    "        cat = catDict['children']\n",
    "    if 'comic' in words:\n",
    "        cat = catDict['comics_graphic']\n",
    "    if 'fantasy' in words:\n",
    "        cat = catDict['fantasy_paranormal']\n",
    "    if 'mystery' in words:\n",
    "        cat = catDict['mystery_thriller_crime']\n",
    "    if 'love' in words:\n",
    "        cat = catDict['young_adult']\n",
    "    predictions.write(l['user_id'] + ',' + l['review_id'] + \",\" + str(cat) + \"\\n\")\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a54a7a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data from Raw File\n",
    "allRatings = []\n",
    "for l in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    allRatings.append(l)\n",
    "ratingsTrain = allRatings[:190000]\n",
    "ratingsValid = allRatings[190000:]\n",
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerItem = defaultdict(list)\n",
    "booksPerUser = defaultdict(set)\n",
    "usersPerBook = defaultdict(set)\n",
    "booksInTrain=set()\n",
    "for u,b,r in ratingsTrain:\n",
    "    ratingsPerUser[u].append((b,r))\n",
    "    ratingsPerItem[b].append((u,r))\n",
    "    booksPerUser[u].add(b)\n",
    "    usersPerBook[b].add(u)\n",
    "    booksInTrain.add(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47a9a196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "0.7062\n"
     ]
    }
   ],
   "source": [
    "### Question 1--------------------------------------------------------------------------------------\n",
    "### ------------------------------------------------------------------------------------------------\n",
    "random.seed(33)#set random seed\n",
    "addpairs=set()\n",
    "addValid=[]\n",
    "interactionValid=[]\n",
    "for u,b,r in ratingsValid:\n",
    "    #build\n",
    "    interactionValid.append((u,b,1))\n",
    "    #add\n",
    "    readbooks=set(ratingsPerUser[u])\n",
    "    unreadbooks=list(booksInTrain-readbooks)\n",
    "    while True:\n",
    "        number=random.randint(0, len(unreadbooks)-1)\n",
    "        addbook=unreadbooks[number]\n",
    "        if (u,addbook) not in addpairs:\n",
    "            addValid.append((u,addbook,0))\n",
    "            addpairs.add((u,addbook))\n",
    "            break\n",
    "interactionValid.extend(addValid)\n",
    "print(len(interactionValid))\n",
    "#calculate acc\n",
    "TrueCount=0\n",
    "for u,b,r in interactionValid:\n",
    "    if b in return1 and r==1:\n",
    "        TrueCount+=1\n",
    "    if b not in return1 and r==0:\n",
    "        TrueCount+=1\n",
    "acc1=TrueCount/len(interactionValid)\n",
    "print(acc1)\n",
    "answers['Q1'] = acc1\n",
    "assertFloat(answers['Q1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1166cc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74745\n"
     ]
    }
   ],
   "source": [
    "### Question 2--------------------------------------------------------------------------------------\n",
    "### ------------------------------------------------------------------------------------------------\n",
    "newreturn1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    newreturn1.add(i)\n",
    "    if count > 3*totalRead/4: break\n",
    "TrueCount=0\n",
    "for u,b,r in interactionValid:\n",
    "    if b in newreturn1 and r==1:\n",
    "        TrueCount+=1\n",
    "    if b not in newreturn1 and r==0:\n",
    "        TrueCount+=1\n",
    "acc2=TrueCount/len(interactionValid)\n",
    "print(acc2)\n",
    "answers['Q2'] = [3/4, acc2]\n",
    "assertFloat(answers['Q2'][0])\n",
    "assertFloat(answers['Q2'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a6d6255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6941\n"
     ]
    }
   ],
   "source": [
    "### Question 3--------------------------------------------------------------------------------------\n",
    "### ------------------------------------------------------------------------------------------------\n",
    "def JaccardSimilarity(set1,set2):\n",
    "    inter=len(set1.intersection(set2))\n",
    "    uni=len(set1.union(set2))\n",
    "    return inter/uni\n",
    "\n",
    "TrueCount=0\n",
    "record=[]\n",
    "for u,b,r in interactionValid:\n",
    "    books=booksPerUser[u]\n",
    "    users=usersPerBook[b]\n",
    "    sim_max=0\n",
    "    for comparedBook in books:\n",
    "        sim_ij=JaccardSimilarity(users,usersPerBook[comparedBook])\n",
    "        if sim_ij>sim_max:\n",
    "            sim_max=sim_ij\n",
    "    record.append(sim_max)\n",
    "    threshold=0.0032679738562091504#48%\n",
    "    if sim_max>threshold and r==1:\n",
    "        TrueCount+=1\n",
    "    if sim_max<threshold and r==0: \n",
    "        TrueCount+=1\n",
    "acc3=TrueCount/len(interactionValid)\n",
    "print(acc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d5031f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pct:95------------\n",
      "0.01834862385321101\n",
      "0.74775\n",
      "pct:96------------\n",
      "0.0196078431372549\n",
      "0.74855\n",
      "pct:97------------\n",
      "0.022222222222222223\n",
      "0.74855\n",
      "pct:98------------\n",
      "0.02631578947368421\n",
      "0.75005\n",
      "pct:99------------\n",
      "0.03571428571428571\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "### Question 4--------------------------------------------------------------------------------------\n",
    "### ------------------------------------------------------------------------------------------------\n",
    "newreturn1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    newreturn1.add(i)\n",
    "    if count > 0.72*totalRead: break\n",
    "import numpy as np\n",
    "record=np.array(record)\n",
    "for p in range(95,100):\n",
    "    print(\"pct:\"+str(p)+\"------------\")\n",
    "    pct=np.percentile(record,p)\n",
    "    print(pct)\n",
    "    TrueCount=0\n",
    "    for u,b,r in interactionValid:\n",
    "        books=booksPerUser[u]\n",
    "        users=usersPerBook[b]\n",
    "        sim_max=0\n",
    "        for comparedBook in books:\n",
    "            sim_ij=JaccardSimilarity(users,usersPerBook[comparedBook])\n",
    "            if sim_ij>sim_max:\n",
    "                sim_max=sim_ij\n",
    "        threshold=pct #0.0032679738562091504#48%\n",
    "        if sim_max>threshold or b in newreturn1:\n",
    "            if r==1:\n",
    "                TrueCount+=1\n",
    "        else:\n",
    "            if r==0: \n",
    "                TrueCount+=1\n",
    "    acc4=TrueCount/len(interactionValid)\n",
    "    print(acc4)\n",
    "    answers['Q3'] = acc3\n",
    "    answers['Q4'] = acc4\n",
    "    assertFloat(answers['Q3'])\n",
    "    assertFloat(answers['Q4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "657d17e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 5--------------------------------------------------------------------------------------\n",
    "### ------------------------------------------------------------------------------------------------\n",
    "# predictions = open(\"predictions_Read.csv\", 'w')\n",
    "import csv\n",
    "with open('predictions_Read.csv', 'w',newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    for l in open(\"pairs_Read.csv\"):\n",
    "        if l.startswith(\"userID\"):\n",
    "            line=['userID','bookID','prediction']\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "        u,b = l.strip().split(',')\n",
    "        books=booksPerUser[u]\n",
    "        users=usersPerBook[b]\n",
    "        sim_max=0\n",
    "        for comparedBook in books:\n",
    "            sim_ij=JaccardSimilarity(users,usersPerBook[comparedBook])\n",
    "            if sim_ij>sim_max:\n",
    "                sim_max=sim_ij\n",
    "        threshold=pct #0.000481000481000481\n",
    "        if sim_max>threshold and b in newreturn1:\n",
    "            predict=1\n",
    "        else:\n",
    "            predict=0\n",
    "        line=[u,b,str(predict)]\n",
    "        writer.writerow(line)\n",
    "\n",
    "answers['Q5'] = \"I confirm that I have uploaded an assignment submission to gradescope\"\n",
    "assert type(answers['Q5']) == str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb0fe6df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10, objective = 0.85552686\n",
      "iteration 20, objective = 0.9204653\n",
      "iteration 30, objective = 0.8344883\n",
      "iteration 40, objective = 0.8592495\n",
      "iteration 50, objective = 0.8455629\n",
      "iteration 60, objective = 0.8529683\n",
      "iteration 70, objective = 0.8808865\n",
      "iteration 80, objective = 0.8695816\n",
      "iteration 90, objective = 0.8509013\n",
      "iteration 100, objective = 0.9021071\n",
      "iteration 110, objective = 0.87423605\n",
      "iteration 120, objective = 0.90187645\n",
      "iteration 130, objective = 0.8553984\n",
      "iteration 140, objective = 0.885085\n",
      "iteration 150, objective = 0.876206\n"
     ]
    }
   ],
   "source": [
    "### Question 9--------------------------------------------------------------------------------------\n",
    "### ------------------------------------------------------------------------------------------------\n",
    "import tensorflow as tf\n",
    "userIDs,itemIDs = {},{}\n",
    "for (u,i,_) in ratingsTrain:\n",
    "#    u,i = d['user_id'],d['book_id']\n",
    "    if not u in userIDs: userIDs[u] = len(userIDs)\n",
    "    if not i in itemIDs: itemIDs[i] = len(itemIDs)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=False,\n",
    "    name='Adam',\n",
    ")\n",
    "\n",
    "class LatentFactorModelBiasOnly(tf.keras.Model):\n",
    "    def __init__(self, mu, lamb):\n",
    "        super(LatentFactorModelBiasOnly, self).__init__()\n",
    "        # Initialize to average\n",
    "        self.alpha = tf.Variable(mu)\n",
    "        # Initialize to small random values\n",
    "        self.betaU = tf.Variable(tf.random.normal([len(userIDs)],stddev=0.001))\n",
    "        self.betaI = tf.Variable(tf.random.normal([len(itemIDs)],stddev=0.001))\n",
    "        self.lamb = lamb\n",
    "\n",
    "    # Prediction for a single instance (useful for evaluation)\n",
    "    def predict(self, u, i):\n",
    "        p = self.alpha + self.betaU[u] + self.betaI[i]\n",
    "        return p\n",
    "\n",
    "    # L2 Regularizer\n",
    "    def reg(self):\n",
    "        return self.lamb * (tf.reduce_sum(self.betaU**2) +\\\n",
    "                            tf.reduce_sum(self.betaI**2))\n",
    "    \n",
    "    # Prediction for a sample of instances\n",
    "    def predictSample(self, sampleU, sampleI):\n",
    "        u = tf.convert_to_tensor(sampleU, dtype=tf.int32)\n",
    "        i = tf.convert_to_tensor(sampleI, dtype=tf.int32)\n",
    "        beta_u = tf.nn.embedding_lookup(self.betaU, u)\n",
    "        beta_i = tf.nn.embedding_lookup(self.betaI, i)\n",
    "        pred = self.alpha + beta_u + beta_i\n",
    "        return pred\n",
    "    \n",
    "    # Loss\n",
    "    def call(self, sampleU, sampleI, sampleR):\n",
    "        pred = self.predictSample(sampleU, sampleI)\n",
    "        r = tf.convert_to_tensor(sampleR, dtype=tf.float32)\n",
    "        return tf.nn.l2_loss(pred - r) / len(sampleR)\n",
    "\n",
    "mu = sum([r for _,_,r in ratingsTrain]) / len(ratingsTrain)\n",
    "modelBiasOnly = LatentFactorModelBiasOnly(mu, 1)# lambda=1\n",
    "\n",
    "def trainingStepBiasOnly(model, interactions):\n",
    "    Nsamples = 5000\n",
    "    with tf.GradientTape() as tape:\n",
    "        sampleU, sampleI, sampleR = [], [], []\n",
    "        for _ in range(Nsamples):\n",
    "            u,i,r = random.choice(interactions)\n",
    "            sampleU.append(userIDs[u])\n",
    "            sampleI.append(itemIDs[i])\n",
    "            sampleR.append(r)\n",
    "\n",
    "        loss = model(sampleU,sampleI,sampleR)\n",
    "        loss += model.reg()\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients((grad, var) for\n",
    "                              (grad, var) in zip(gradients, model.trainable_variables)\n",
    "                              if grad is not None)\n",
    "    return loss.numpy()\n",
    "\n",
    "for i in range(150):\n",
    "    obj = trainingStepBiasOnly(modelBiasOnly, ratingsTrain)\n",
    "    if (i % 10 == 9): print(\"iteration \" + str(i+1) + \", objective = \" + str(obj))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf7d19e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6802113179223874\n",
      "1.6800486872872697\n",
      "Special Case:33\n"
     ]
    }
   ],
   "source": [
    "#MSE Function\n",
    "def MSE(pred, label):\n",
    "    err_sqr = [(x-y)**2 for x,y in zip(pred,label)]\n",
    "    return sum(err_sqr) / len(err_sqr)\n",
    "alwaysPredictMean = [mu for _ in ratingsValid]\n",
    "labels = [r for _,_,r in ratingsValid]\n",
    "print(MSE(alwaysPredictMean, labels))\n",
    "biasOnlyPredictions=[]\n",
    "case=0\n",
    "for u,i,_ in ratingsValid:\n",
    "    if u not in userIDs or i not in itemIDs:\n",
    "        biasOnlyPredictions.append(mu)\n",
    "        case+=1\n",
    "    else:\n",
    "        biasOnlyPredictions.append(modelBiasOnly.predict(userIDs[u],itemIDs[i]).numpy())\n",
    "\n",
    "validMSE=MSE(biasOnlyPredictions, labels)\n",
    "print(validMSE)\n",
    "print(\"Special Case:{}\".format(case))\n",
    "answers['Q9'] = validMSE\n",
    "assertFloat(answers['Q9'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca1438ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5605, 9952, 0.00063451496, -0.00085635635]\n",
      "['u59851955', 'u13130392', 0.00063451496, -0.00085635635]\n"
     ]
    }
   ],
   "source": [
    "### Question 10--------------------------------------------------------------------------------------\n",
    "### ------------------------------------------------------------------------------------------------\n",
    "betaU_lst=modelBiasOnly.betaU.numpy()\n",
    "maxBeta,minBeta=0,0\n",
    "for i,j in enumerate(betaU_lst):\n",
    "    if j>maxBeta:\n",
    "        maxBeta=j\n",
    "        maxUser=i\n",
    "    if j<minBeta:\n",
    "        minBeta=j\n",
    "        minUser=i\n",
    "print([maxUser, minUser, maxBeta, minBeta])\n",
    "userIDs,itemIDs = {},{}\n",
    "for (u,i,_) in ratingsTrain:\n",
    "#     u,i = d['user_id'],d['book_id']\n",
    "    if not u in userIDs: \n",
    "        userIDs[u] = len(userIDs)\n",
    "        if len(userIDs)==maxUser:\n",
    "            maxUserNew=u\n",
    "        if len(userIDs)==minUser:\n",
    "            minUserNew=u\n",
    "print([maxUserNew, minUserNew, maxBeta, minBeta])\n",
    "answers['Q10'] = [maxUserNew, minUserNew, float(maxBeta), float(minBeta)]\n",
    "assert [type(x) for x in answers['Q10']] == [str, str, float, float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58045df8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training \n",
      "Train Model when Lambda = 1e-10\n",
      "iteration 10, objective = 0.8482036\n",
      "iteration 20, objective = 0.88153845\n",
      "iteration 30, objective = 0.8586732\n",
      "iteration 40, objective = 0.8779785\n",
      "iteration 50, objective = 0.8389094\n",
      "iteration 60, objective = 0.9073632\n",
      "iteration 70, objective = 0.79359925\n",
      "iteration 80, objective = 0.845535\n",
      "iteration 90, objective = 0.82839453\n",
      "iteration 100, objective = 0.8167258\n",
      "Training Finished\n",
      "MSE on ValidationSet:1.616265688257142\n",
      "Start Training \n",
      "Train Model when Lambda = 1e-08\n",
      "iteration 10, objective = 0.85976416\n",
      "iteration 20, objective = 0.8482876\n",
      "iteration 30, objective = 0.86354154\n",
      "iteration 40, objective = 0.8547682\n",
      "iteration 50, objective = 0.82773983\n",
      "iteration 60, objective = 0.8273632\n",
      "iteration 70, objective = 0.8091401\n",
      "iteration 80, objective = 0.8022107\n",
      "iteration 90, objective = 0.8376659\n",
      "iteration 100, objective = 0.852623\n",
      "Training Finished\n",
      "MSE on ValidationSet:1.6065778402853894\n",
      "Start Training \n",
      "Train Model when Lambda = 1e-06\n",
      "iteration 10, objective = 0.8247099\n",
      "iteration 20, objective = 0.8647592\n",
      "iteration 30, objective = 0.84034365\n",
      "iteration 40, objective = 0.8743697\n",
      "iteration 50, objective = 0.8175621\n",
      "iteration 60, objective = 0.8399911\n",
      "iteration 70, objective = 0.8515124\n",
      "iteration 80, objective = 0.8089444\n",
      "iteration 90, objective = 0.7985937\n",
      "iteration 100, objective = 0.8336554\n",
      "Training Finished\n",
      "MSE on ValidationSet:1.5970534823067588\n",
      "Start Training \n",
      "Train Model when Lambda = 1e-05\n",
      "iteration 10, objective = 0.8602145\n",
      "iteration 20, objective = 0.8421272\n",
      "iteration 30, objective = 0.8607698\n",
      "iteration 40, objective = 0.8217247\n",
      "iteration 50, objective = 0.8139728\n",
      "iteration 60, objective = 0.8689007\n",
      "iteration 70, objective = 0.8428808\n",
      "iteration 80, objective = 0.8439295\n",
      "iteration 90, objective = 0.819925\n",
      "iteration 100, objective = 0.81131846\n",
      "Training Finished\n",
      "MSE on ValidationSet:1.5920930827596471\n",
      "Start Training \n",
      "Train Model when Lambda = 0.0001\n",
      "iteration 10, objective = 0.8668934\n",
      "iteration 20, objective = 0.8669921\n",
      "iteration 30, objective = 0.84602606\n",
      "iteration 40, objective = 0.83430123\n",
      "iteration 50, objective = 0.8590899\n",
      "iteration 60, objective = 0.8209984\n",
      "iteration 70, objective = 0.84570915\n",
      "iteration 80, objective = 0.836259\n",
      "iteration 90, objective = 0.8083027\n",
      "iteration 100, objective = 0.8014238\n",
      "Training Finished\n",
      "MSE on ValidationSet:1.5948313584981268\n",
      "Start Training \n",
      "Train Model when Lambda = 0.001\n",
      "iteration 10, objective = 0.86077464\n",
      "iteration 20, objective = 0.8517452\n",
      "iteration 30, objective = 0.8692245\n",
      "iteration 40, objective = 0.83861005\n",
      "iteration 50, objective = 0.84088683\n",
      "iteration 60, objective = 0.84337497\n",
      "iteration 70, objective = 0.8898061\n",
      "iteration 80, objective = 0.8785304\n",
      "iteration 90, objective = 0.8162328\n",
      "iteration 100, objective = 0.83133745\n",
      "Training Finished\n",
      "MSE on ValidationSet:1.6305848830743734\n",
      "Start Training \n",
      "Train Model when Lambda = 0.01\n",
      "iteration 10, objective = 0.871333\n",
      "iteration 20, objective = 0.8693745\n",
      "iteration 30, objective = 0.87544525\n",
      "iteration 40, objective = 0.8888533\n",
      "iteration 50, objective = 0.8532336\n",
      "iteration 60, objective = 0.8492643\n",
      "iteration 70, objective = 0.8797507\n",
      "iteration 80, objective = 0.85401666\n",
      "iteration 90, objective = 0.8713892\n",
      "iteration 100, objective = 0.8770766\n",
      "Training Finished\n",
      "MSE on ValidationSet:1.6688512603153838\n",
      "Start Training \n",
      "Train Model when Lambda = 0.1\n",
      "iteration 10, objective = 0.8357474\n",
      "iteration 20, objective = 0.9064558\n",
      "iteration 30, objective = 0.8568547\n",
      "iteration 40, objective = 0.862102\n",
      "iteration 50, objective = 0.8453243\n",
      "iteration 60, objective = 0.8931256\n",
      "iteration 70, objective = 0.87812376\n",
      "iteration 80, objective = 0.8171317\n",
      "iteration 90, objective = 0.8727618\n",
      "iteration 100, objective = 0.871206\n",
      "Training Finished\n",
      "MSE on ValidationSet:1.6788491488884432\n",
      "Start Training \n",
      "Train Model when Lambda = 10\n",
      "iteration 10, objective = 0.9970953\n",
      "iteration 20, objective = 1.0226995\n",
      "iteration 30, objective = 0.93182194\n",
      "iteration 40, objective = 0.88181686\n",
      "iteration 50, objective = 0.8577048\n",
      "iteration 60, objective = 0.85094863\n",
      "iteration 70, objective = 0.867304\n",
      "iteration 80, objective = 0.90514016\n",
      "iteration 90, objective = 0.87396085\n",
      "iteration 100, objective = 0.8586934\n",
      "Training Finished\n",
      "MSE on ValidationSet:1.6802022226915385\n"
     ]
    }
   ],
   "source": [
    "### Question 11--------------------------------------------------------------------------------------\n",
    "### ------------------------------------------------------------------------------------------------\n",
    "for Lambda in [1e-10,1e-8,1e-6,1e-5,1e-4,1e-3,0.01, 0.1, 10]: \n",
    "    userIDs,itemIDs = {},{}\n",
    "    for (u,i,_) in ratingsTrain:\n",
    "    #    u,i = d['user_id'],d['book_id']\n",
    "        if not u in userIDs: userIDs[u] = len(userIDs)\n",
    "        if not i in itemIDs: itemIDs[i] = len(itemIDs)\n",
    "    modelBiasOnlylambda = LatentFactorModelBiasOnly(mu, Lambda)\n",
    "    print(\"Start Training \\nTrain Model when Lambda = {}\".format(Lambda))\n",
    "    for i in range(100):\n",
    "        obj = trainingStepBiasOnly(modelBiasOnlylambda, ratingsTrain)\n",
    "        if (i % 10 == 9): print(\"iteration \" + str(i+1) + \", objective = \" + str(obj))\n",
    "    print(\"Training Finished\")\n",
    "    biasOnlyPredictions=[]\n",
    "    for u,i,_ in ratingsValid:\n",
    "        if u not in userIDs or i not in itemIDs:\n",
    "            biasOnlyPredictions.append(mu)\n",
    "        else:\n",
    "            biasOnlyPredictions.append(modelBiasOnlylambda.predict(userIDs[u],itemIDs[i]).numpy())\n",
    "    validMSELambda=MSE(biasOnlyPredictions, labels)\n",
    "    print(\"MSE on ValidationSet:{}\".format(validMSELambda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9422f16f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training \n",
      "Train Model when Lambda = 0.0001\n",
      "iteration 10, objective = 0.8787938\n",
      "iteration 20, objective = 0.8612392\n",
      "iteration 30, objective = 0.8280699\n",
      "iteration 40, objective = 0.85366464\n",
      "iteration 50, objective = 0.8055558\n",
      "iteration 60, objective = 0.8140958\n",
      "iteration 70, objective = 0.8404159\n",
      "iteration 80, objective = 0.85555756\n",
      "iteration 90, objective = 0.8126107\n",
      "iteration 100, objective = 0.8084058\n",
      "Training Finished\n",
      "MSE on ValidationSet:1.580234437670107\n"
     ]
    }
   ],
   "source": [
    "Lambda=1e-5\n",
    "\n",
    "userIDs,itemIDs = {},{}\n",
    "for (u,i,_) in ratingsTrain:\n",
    "#    u,i = d['user_id'],d['book_id']\n",
    "    if not u in userIDs: userIDs[u] = len(userIDs)\n",
    "    if not i in itemIDs: itemIDs[i] = len(itemIDs)\n",
    "modelBiasOnlylambda = LatentFactorModelBiasOnly(mu, Lambda)\n",
    "print(\"Start Training \\nTrain Model when Lambda = {}\".format(Lambda))\n",
    "for i in range(100):\n",
    "    obj = trainingStepBiasOnly(modelBiasOnlylambda, ratingsTrain)\n",
    "    if (i % 10 == 9): print(\"iteration \" + str(i+1) + \", objective = \" + str(obj))\n",
    "print(\"Training Finished\")\n",
    "biasOnlyPredictions=[]\n",
    "for u,i,_ in ratingsValid:\n",
    "    if u not in userIDs or i not in itemIDs:\n",
    "        biasOnlyPredictions.append(mu)\n",
    "    else:\n",
    "        biasOnlyPredictions.append(modelBiasOnlylambda.predict(userIDs[u],itemIDs[i]).numpy())\n",
    "validMSELambda=MSE(biasOnlyPredictions, labels)\n",
    "print(\"MSE on ValidationSet:{}\".format(validMSELambda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3d870b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special Case:27\n"
     ]
    }
   ],
   "source": [
    "(lamb, validMSE)=(1e-05,1.5920930827596471)\n",
    "answers['Q11'] = (lamb, validMSE)\n",
    "assertFloat(answers['Q11'][0])\n",
    "assertFloat(answers['Q11'][1])\n",
    "case=0\n",
    "userIDs,itemIDs = {},{}\n",
    "for (u,i,_) in ratingsTrain:\n",
    "    if not u in userIDs: userIDs[u] = len(userIDs)\n",
    "    if not i in itemIDs: itemIDs[i] = len(itemIDs)\n",
    "with open('predictions_Rating.csv', 'w',newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    for l in open(\"pairs_Rating.csv\"):\n",
    "        if l.startswith(\"userID\"):\n",
    "            line=['userID','bookID','prediction']\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "        u,b = l.strip().split(',') # Read the user and item from the \"pairs\" file and write out your prediction\n",
    "        if u not in userIDs or i not in itemIDs:\n",
    "            predict=mu\n",
    "            case+=1\n",
    "        else:\n",
    "            predict=modelBiasOnlylambda.predict(userIDs[u],itemIDs[i]).numpy()\n",
    "        line=[u,b,str(predict)]\n",
    "        writer.writerow(line)\n",
    "    \n",
    "print(\"Special Case:{}\".format(case))\n",
    "f = open(\"answers_hw3.txt\", 'w')\n",
    "f.write(str(answers) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073b4336",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSTM_NN",
   "language": "python",
   "name": "lstm_cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
